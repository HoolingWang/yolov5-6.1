# Ultralytics YOLOv5 ğŸš€, AGPL-3.0 license
"""
Run YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.

Usage - sources:
    $ python detect.py --weights yolov5s.pt --source 0                               # webcam
                                                     img.jpg                         # image
                                                     vid.mp4                         # video
                                                     screen                          # screenshot
                                                     path/                           # directory
                                                     list.txt                        # list of images
                                                     list.streams                    # list of streams
                                                     'path/*.jpg'                    # glob
                                                     'https://youtu.be/LNwODJXcvt4'  # YouTube
                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream

Usage - formats:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle
"""
# å¯¼å…¥ç›¸å…³æ•°æ®åº“
from utils.torch_utils import select_device, smart_inference_mode
from utils.general import (
    LOGGER,
    Profile,
    check_file,
    check_img_size,
    check_imshow,
    check_requirements,
    colorstr,
    cv2,
    increment_path,
    non_max_suppression,
    print_args,
    scale_boxes,
    strip_optimizer,
    xyxy2xywh,
)
from utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams
from models.common import DetectMultiBackend
from ultralytics.utils.plotting import Annotator, colors, save_one_box
import argparse  # è§£æç›®å½•è¡Œå‚æ•°
import csv
import os
import platform
import sys
from pathlib import Path

import torch

# è®¾ç½®è·¯å¾„ï¼Œè·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå¹¶ä½¿ç”¨PATHæ–¹æ³•æ·»åŠ åˆ°Pathå¯¹è±¡
FILE = Path(__file__).resolve()
# __file__: è¿™æ˜¯ä¸€ä¸ªå†…ç½®å˜é‡ï¼ŒåŒ…å«å½“å‰æ­£åœ¨æ‰§è¡Œçš„ Python è„šæœ¬çš„è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼Œå–å†³äºæ‰§è¡Œæ–¹å¼ï¼‰ã€‚
# Path(__file__): é€šè¿‡å°† \_\_file\_\_ ä¼ é€’ç»™ Path ç±»ï¼Œåˆ›å»ºä¸€ä¸ª Path å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡è¡¨ç¤ºå½“å‰æ–‡ä»¶çš„è·¯å¾„ã€‚
# .resolve()fuzeå°†è·¯å¾„è½¬åŒ–ä¸ºç»å¯¹è·¯å¾„
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

# æ·»åŠ è‡ªå®šä¹‰æ¨¡å—


@smart_inference_mode()
def run(
    # è½½å…¥å‚æ•°
    weights=ROOT / "yolov5s.pt",  # model path or triton URL   æƒé‡æ–‡ä»¶è·¯å¾„
    source=ROOT / "data/images",  # file/dir/URL/glob/screen/0(webcam)   è¾“å…¥å›¾åƒçš„è·¯å¾„
    data=ROOT / "data/coco128.yaml",  # dataset.yaml path   æ•°æ®é›†çš„é…ç½®æ–‡ä»¶
    imgsz=(640, 640),  # inference size (height, width)   
    conf_thres=0.25,  # confidence threshold   ç½®ä¿¡åº¦é˜ˆå€¼
    iou_thres=0.45,  # NMS IOU threshold   é…æå¤§å€¼æŠ‘åˆ¶çš„ioué˜ˆå€¼
    max_det=1000,  # maximum detections per image   æ¯å¼ å›¾åƒçš„æœ€å¤§æ£€æµ‹æ¡†æ•°é‡
    device="",  # cuda device, i.e. 0 or 0,1,2,3 or cpu   è®¾å¤‡
    view_img=False,  # show results   æ˜¯å¦æ‰“å°æ£€æµ‹ç»“æœå›¾åƒ
    save_txt=False,  # save results to *.txt   æ˜¯å¦å°†æ£€æµ‹ç»“æœä¿å­˜ä¸ºtxtæ–‡ä»¶
    save_csv=False,  # save results in CSV format   æ˜¯å¦ä¿å­˜ä¸ºcsvæ ¼å¼
    save_conf=False,  # save confidences in --save-txt labels   æ˜¯å¦åœ¨æ–‡æœ¬æ–‡ä»¶åŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯
    save_crop=False,  # save cropped prediction boxes   æ˜¯å¦å°†æ£€æµ‹å‡ºç›®æ ‡çš„åŒºåŸŸä¿å­˜ä¸ºå›¾åƒæ–‡ä»¶
    nosave=False,  # do not save images/videos   æ˜¯å¦ä¿å­˜å›¾åƒæˆ–è€…è§†é¢‘
    classes=None,  # filter by class: --class 0, or --class 0 2 3   æŒ‡å®šè¦æ£€æµ‹çš„ç±»åˆ«ï¼ˆä¸€ç±»æˆ–å¤šç±»ï¼‰
    agnostic_nms=False,  # class-agnostic NMS   æ˜¯å¦ä½¿ç”¨ç±»åˆ«æ— å…³çš„éæå¤§å€¼æŠ‘åˆ¶
    augment=False,  # augmented inference   æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼ºçš„æ–¹å¼è¿›è¡Œç›®æ ‡æ£€æµ‹
    visualize=False,  # visualize features   æ˜¯å¦å¯è§†åŒ–æ¨¡å‹ä¸­çš„ç‰¹å¾å›¾
    update=False,  # update all models   æ˜¯å¦è‡ªåŠ¨æ›´æ–°æ¨¡å‹æƒé‡æ–‡ä»¶
    project=ROOT / "runs/detect",  # save results to project/name   ç»“æœä¿å­˜æ–‡ä»¶å¤¹
    name="exp",  # save results to project/name   ç»“æœä¿å­˜æ–‡ä»¶å¤¹ï¼ˆä¸å‰è€…æ‹¼æ¥ï¼‰
    exist_ok=False,  # existing project/name ok, do not increment   æ˜¯å¦è¦†ç›–ä¿å­˜ç»“æœçš„æ–‡ä»¶å¤¹
    line_thickness=3,  # bounding box thickness (pixels)   æ£€æµ‹æ¡†çº¿æ¡å®½åº¦
    hide_labels=False,  # hide labels   æ˜¯å¦éšè—æ ‡ç­¾ä¿¡æ¯
    hide_conf=False,  # hide confidences   æ˜¯å¦éšè—ç½®ä¿¡åº¦
    half=False,  # use FP16 half-precision inference   æ˜¯å¦ä½¿ç”¨FP16åŠç²¾åº¦è®­ç»ƒ
    dnn=False,  # use OpenCV DNN for ONNX inference   æ˜¯å¦ä½¿ç”¨opencvçš„dnnä½œä¸ºonnxçš„åç«¯
    vid_stride=1,  # video frame-rate stride   è§†é¢‘æµæ£€æµ‹æ­¥é•¿ï¼ˆé—´éš”å¸§æ•°ï¼‰
):
    # åˆå§‹åŒ–é…ç½®
    # è¾“å…¥è·¯å¾„è½¬åŒ–ä¸º
    source = str(source)
    # æ˜¯å¦ä¿å­˜å›¾ç‰‡å’Œtxtæ–‡ä»¶ï¼Œå¦‚æœåŒæ—¶ä¸ºtrueåˆ™ä¿å­˜å›¾åƒ
    save_img = not nosave and not source.endswith(
        ".txt")  # save inference images
    # åˆ¤æ–­sourceæ˜¯ä¸æ˜¯è§†é¢‘ã€å›¾åƒæ–‡ä»¶
    # Path()æå–æ–‡ä»¶åï¼Œ.suffixè·å–æœ€åä¸€ä¸ªç»„ä»¶çš„æ–‡ä»¶æ‰©å±•å
    # pythonä¸­ä¸€èˆ¬ä½¿ç”¨pathlib.Path().suffixå¯¹è·¯å¾„è¿›è¡Œåˆ‡ç‰‡ï¼Œç›´æ¥è·å–.åç¼€æ–¹ä¾¿å¤„ç†
    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)
    # åˆ¤æ–­sourceæ˜¯å¦ä¸ºé“¾æ¥
    # .lower()è½¬åŒ–ä¸ºå°å†™ï¼›.upper()è½¬åŒ–ä¸ºå¤§å†™ï¼›.title()è½¬åŒ–ä¸ºé¦–å­—æ¯å¤§å†™
    is_url = source.lower().startswith(("rtsp://", "rtmp://", "http://", "https://"))
    # åˆ¤æ–­sourceæ˜¯å¦ä¸ºæ‘„åƒå¤´
    # .isnumeric()æ˜¯å¦ç”±æ•°å­—ç»„æˆ
    webcam = source.isnumeric() or source.endswith(
        ".streams") or (is_url and not is_file)
    screenshot = source.lower().startswith("screen")
    # æ£€æŸ¥æ˜¯ä¸æ˜¯ä¸€ä¸ªå›¾åƒæˆ–è€…è§†é¢‘çš„é“¾æ¥ï¼Œå¦‚æœæ˜¯çš„è¯ä¸‹è½½
    if is_url and is_file:
        source = check_file(source)  # download

    # Directories
    # save_diræ˜¯ä¿å­˜è¿è¡Œç»“æœçš„æ–‡ä»¶å¤¹åç§°ï¼Œé€šè¿‡é€’å¢çš„æ–¹å¼æ¥å‘½å
    save_dir = increment_path(Path(project) / name,
                              exist_ok=exist_ok)  # increment run
    # å¦‚æœéœ€è¦ä¿å­˜txtæ–‡ä»¶å°±åœ¨åˆ›å»ºçš„ä¿å­˜æ–‡ä»¶å¤¹ä¸‹é¢åˆ›å»ºlabelsæ–‡ä»¶å¤¹
    (save_dir / "labels" if save_txt else save_dir).mkdir(parents=True,
                                                          exist_ok=True)  # make dir

    # Load model åŠ è½½æ¨¡å‹
    # åœ¨å‚æ•°é…ç½®éƒ¨åˆ†é€‰æ‹©ä½ çš„è®¾å¤‡,
    device = select_device(device)
    # æ¨¡å‹åŠ è½½ï¼Œè¾“å…¥ä¿¡æ¯åŒ…å«æƒé‡æ–‡ä»¶ï¼Œè®¾å¤‡ä¿¡æ¯ï¼Œè¯¦æƒ…å‚è€ƒcommonæ–‡ä»¶DetectMultiBackendå‡½æ•°çš„å®ç°
    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)
    stride, names, pt = model.stride, model.names, model.pt
    # '''
    # stride:æ¨ç†ç”¨åˆ°çš„æ­¥é•¿ï¼Œé»˜è®¤32ï¼Œå¤§æ­¥é•¿èŠ‚çœç©ºé—´æé«˜æ•ˆç‡ï¼Œå°æ­¥é•¿å¯¹äºå°ç›®æ ‡æ£€æµ‹æ€§èƒ½æœ‰å¸®åŠ©ï¼Œæ¨¡å‹è¾¹ç•Œæ›´åŠ ç»†è‡´
    # name:ä¿å­˜æ¨ç†ç»“æœåçš„åˆ—è¡¨
    # ptï¼šæ˜¯å¦åŠ è½½pytorchæ¨¡å‹
    # '''
    imgsz = check_img_size(imgsz, s=stride)  # check image size å°†è¾“å…¥å°ºå¯¸è½¬åŒ–ä¸ºå¯è¢«strideæ•´é™¤çš„å°ºå¯¸

    # Dataloader   åŠ è½½æ•°æ®
    bs = 1  # batch_size
    if webcam:   # è§†é¢‘ä¿¡æ¯
        view_img = check_imshow(warn=True)   # check_imshowæ£€æŸ¥å½“å‰ç¯å¢ƒæ˜¯å¦æ”¯æŒå›¾åƒæ˜¾ç¤ºåŠŸèƒ½ã€‚
        # LoadStreams ç±»ç”¨äºå¤„ç†è§†é¢‘æµæ•°æ®ï¼Œè¿”å›æ ¼å¼ä¸ºæºã€å¤„ç†åçš„å›¾åƒã€åŸå§‹å›¾åƒç­‰ã€‚
        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)
        '''
        source: è¾“å…¥æ•°æ®æºï¼›
        image_size: å›¾ç‰‡è¯†åˆ«å‰è¢«ç¼©æ”¾çš„å¤§å°ï¼›
        stride: è¯†åˆ«æ—¶çš„æ­¥é•¿ï¼›
        atuo: æ˜¯å¦å°†å›¾åƒå¡«å……ä¸ºæ­£æ–¹å½¢ï¼ŒTrueè¡¨ç¤ºä¸éœ€è¦ã€‚
        '''
        bs = len(dataset)  # batch_sizeæ‰¹å¤§å°
    elif screenshot:
        dataset = LoadScreenshots(
            source, img_size=imgsz, stride=stride, auto=pt)
    else:   # ç›´æ¥è¯»å–å›¾åƒ
        dataset = LoadImages(source, img_size=imgsz,
                             stride=stride, auto=pt, vid_stride=vid_stride)
    vid_path, vid_writer = [None] * bs, [None] * \
        bs   # å‰è€…æ˜¯ä¿å­˜è§†é¢‘çš„è·¯å¾„ï¼Œåè€…æ˜¯ä¸€ä¸ªcv2.VideoWriterå¯¹è±¡

    # Run inference å¼€å§‹æ¨ç†
    # warmup åŠ è½½æ¨¡å‹è¾“å…¥ä¸€å¼ ç©ºå›¾åƒåˆå§‹åŒ–æ¨¡å‹
    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))
    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))
    '''
    seenï¼šç”¨äºè®°å½•è¿›ç¨‹ï¼Œå½“å‰æœ‰å¤šå°‘å›¾ç‰‡è¢«å¤„ç†
    dtï¼šå­˜å‚¨æ¯ä¸€æ­¥çš„è€—æ—¶
    '''
    for path, im, im0s, vid_cap, s in dataset:
        '''
        åœ¨datasetä¸­ï¼Œæ¯æ¬¡è¿­ä»£è¿”å›çš„å€¼ä¸ºself.source, img, img0, None, ''
        path : æ–‡ä»¶è·¯å¾„ï¼ˆå³sourceï¼‰
        im : resiseåçš„å›¾åƒ
        im0s : åŸå§‹å›¾åƒ
        vid_cap = none
        s : å›¾ç‰‡çš„åŸºæœ¬ä¿¡æ¯ï¼Œæ¯”å¦‚è·¯å¾„å¤§å°
        '''
        with dt[0]:   # è®°å½•å›¾åƒé¢„å¤„ç†çš„æ—¶é—´
            im = torch.from_numpy(im).to(model.device)   #nunpy->torch
            im = im.half() if model.fp16 else im.float()  # uint8 to im.half()->fp16(2å­—èŠ‚)/om.float()->32(4å­—èŠ‚)
            im /= 255  # 0 - 255 to 0.0 - 1.0
            if len(im.shape) == 3:
                im = im[None]  # expand for batch dim
                '''
                im[None]æ˜¯ä¸€ç§ä½¿ç”¨Noneæ–¹æ³•æ¥å¢åŠ å‘é‡ç»´åº¦çš„æ–¹æ³•ï¼Œç›¸å½“äºim.unsqueeze(0), im.view(1, *im.shape)
                [C, H, W] -> [B, C, H, W]
                '''
            if model.xml and im.shape[0] > 1: 
                ims = torch.chunk(im, im.shape[0], 0)
                '''
                torch.chunk æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæ²¿æŒ‡å®šç»´åº¦å°†å¼ é‡åˆ†å‰²æˆå¤šä¸ªå­å¼ é‡ã€‚
                torch.chunk(im, im.shape[0], 0) å°† im æ²¿ç¬¬0ç»´ï¼ˆæ‰¹æ¬¡ç»´åº¦ï¼‰åˆ†å‰²æˆ im.shape[0] ä¸ªå­å¼ é‡ã€‚
                è¿™æ„å‘³ç€å¦‚æœ im çš„å½¢çŠ¶æ˜¯ (N, C, H, W)ï¼Œåˆ™ä¼šå¾—åˆ° N ä¸ªå½¢çŠ¶ä¸º (1, C, H, W) çš„å­å¼ é‡ã€‚
                '''

        # Inference
        with dt[1]:   # æ¨ç†æ—¶é—´
            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False
            if model.xml and im.shape[0] > 1:
                pred = None
                for image in ims:
                    if pred is None:
                        # å¯¹æ¯å¼ å›¾åƒè¿›è¡Œæ¨ç†ï¼Œå¹¶å¢åŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ä»¥ä¾¿åç»­æ‹¼æ¥
                        # [num_detections, detection_info] -> [1, num_detections, detection_info]
                        # num_detectionsè¡¨ç¤ºæ£€æµ‹æ¡†çš„æ•°é‡ï¼Œ detection_infoè¡¨ç¤ºæ£€æµ‹æ¡†çš„ä¿¡æ¯'x, y, w, h, confidence, class'
                        pred = model(image, augment=augment,
                                     visualize=visualize).unsqueeze(0)   #
                    else:
                        # [1, num_detections, detection_info] -> [batch_size, num_detections, detection_info]
                        pred = torch.cat(
                            (pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)
                pred = [pred, None]
            else:
                pred = model(im, augment=augment, visualize=visualize)   # ç›´æ¥æ¨ç†

        # NMS ä½¿ç”¨éæå¤§å€¼æŠ‘åˆ¶å»é™¤å¤šä½™çš„æ£€æµ‹æ¡†
        with dt[2]:   # éæå¤§å€¼æŠ‘åˆ¶æ—¶é—´
            pred = non_max_suppression(
                pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)
            '''
            predï¼šç½‘ç»œçš„è¾“å‡ºç»“æœ
            conf_thresï¼šç½®ä¿¡åº¦é˜ˆå€¼
            iou_thresï¼šioué˜ˆå€¼
            classesï¼šæ˜¯å¦åªä¿ç•™ç‰¹å®šç±»åˆ« é»˜è®¤ä¸ºNone
            agnostic_nums: è¿›è¡Œnmsæ˜¯å¦ä¹Ÿå»é™¤ä¸åŒç±»æ¯”çš„æ¡†
            max_detï¼šæ£€æµ‹æ¡†æœ€å¤§çš„æ•°é‡
            '''

        # Second-stage classifier (optional)
        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)

        # Define the path for the CSV file CSVæ–‡ä»¶è·¯å¾„
        csv_path = save_dir / "predictions.csv"

        # Create or append to the CSV file
        def write_to_csv(image_name, prediction, confidence):
            """Writes prediction data for an image to a CSV file, appending if the file exists."""
            data = {"Image Name": image_name, "Prediction": prediction,
                    "Confidence": confidence}  # CSVæ–‡ä»¶å†™å…¥çš„å†…å®¹
            with open(csv_path, mode="a", newline="") as f:
                # csv.DictWriter åˆ›å»ºä¸€ä¸ªå†™å­—å…¸æ•°æ®çš„ CSV å†™å…¥å™¨ï¼Œfieldnames å‚æ•°æŒ‡å®šå­—æ®µåç§°ã€‚
                writer = csv.DictWriter(f, fieldnames=data.keys())
                if not csv_path.is_file():  # å¦‚æœæ˜¯ç¬¬ä¸€è¡Œå°±éœ€è¦å†™å…¥è¡¨å¤´
                    writer.writeheader()
                writer.writerow(data)  # å†™å…¥å…·ä½“å†…å®¹

        # Process predictions
        for i, det in enumerate(pred):  # per image ï¼Œæ¯æ¬¡è¿­ä»£å¤„ç†ä¸€å¼ å›¾ç‰‡
            '''
            iï¼šæ¯ä¸ªbatchçš„ä¿¡æ¯
            detï¼šè¡¨ç¤º5ä¸ªæ£€æµ‹æ¡†çš„ä¿¡æ¯
            '''
            seen += 1  # è®¡æ•°åŠŸèƒ½
            if webcam:  # batch_size >= 1
                # å¦‚æœè¾“å…¥æºæ˜¯wecamåˆ™batch_size >= 1ï¼Œå–å‡ºdatasetä¸­çš„ä¸€å¼ å›¾ç‰‡
                p, im0, frame = path[i], im0s[i].copy(), dataset.count
                s += f"{i}: "  # såé¢æ‹¼æ¥ä¸€ä¸ªå­—ç¬¦ä¸²i
            else:
                p, im0, frame = path, im0s.copy(), getattr(dataset, "frame", 0)
                '''
                LoadImageæµæ°´è¯»å–æ–‡æœ¬æ–‡ä»¶ä¸­çš„ç…§ç‰‡æˆ–è€…è§†é¢‘ batch_size = 1
                p: å½“å‰å›¾ç‰‡/è§†é¢‘çš„ç»å¯¹è·¯å¾„
                s: è¾“å‡ºä¿¡æ¯ åŸå§‹ä¸º''
                im0: åŸå§‹å›¾ç‰‡ letter + pad ä¹‹å‰çš„å›¾ç‰‡
                frame: è§†é¢‘å…­ï¼Œæ­¤æ¬¡å–å¾—æ˜¯ç¬¬å‡ å¼ å›¾ç‰‡
                '''

            p = Path(p)  # to Path
            # å›¾ç‰‡æˆ–è§†é¢‘çš„ä¿å­˜è·¯å¾„
            save_path = str(save_dir / p.name)  # im.jpg
            # ä¿å­˜åæ ‡æ¡†çš„txtæ–‡ä»¶è·¯å¾„
            txt_path = str(save_dir / "labels" / p.stem) + \
                ("" if dataset.mode == "image" else f"_{frame}")  # im.txt
            # è®¾ç½®è¾“å‡ºå›¾ç‰‡ä¿¡æ¯ã€‚å›¾ç‰‡shapeï¼ˆwï¼Œ hï¼‰
            s += "%gx%g " % im.shape[2:]  # print string   %g æ˜¯ä¸€ç§æ ¼å¼åŒ–æµ®ç‚¹æ•°çš„æ–¹å¼
            # normalization gain whwh
            # åŸå›¾çš„å®½å’Œé«˜
            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]
            # ä¿å­˜æˆªå›¾ã€‚å¦‚æœsave_cropä¸ºTrueåˆ™ä¿å­˜æ£€æµ‹æ¡†çš„æˆªå›¾
            imc = im0.copy() if save_crop else im0  # for save_crop
            # å®ä¾‹åŒ–ä¸€ä¸ªç»˜å›¾çš„ç±»ï¼Œç±»ä¸­é¢„å…ˆå­˜å‚¨äº†åŸå›¾ã€çº¿æ¡å®½åº¦ã€ç±»å
            annotator = Annotator(im0, line_width=line_thickness, example=str(names))
            # åˆ¤æ–­æœ‰æ— æ£€æµ‹æ¡†
            if len(det):   # å•ç‹¬å¤„ç†æ¯ä¸ªæ£€æµ‹æ¡†
                # Rescale boxes from img_size to im0 size
                # å°†é¢„æµ‹æ¡†çš„å°ºå¯¸è°ƒæ•´ä¸ºåŸå›¾å¤§å°ï¼ˆå› ä¸ºåŸå›¾åœ¨æ£€æµ‹æ—¶å¯èƒ½ä¼šè¢«è°ƒæ•´å¤§å°ï¼‰
                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()

                # Print results
                # æ‰“å°æ£€æµ‹åˆ°çš„ç±»åˆ«æ•°é‡
                for c in det[:, 5].unique():
                    n = (det[:, 5] == c).sum()  # detections per class
                    # add to string
                    s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "

                # Write results
                for *xyxy, conf, cls in reversed(det):
                    c = int(cls)  # integer class
                    label = names[c] if hide_conf else f"{names[c]}"
                    confidence = float(conf)
                    confidence_str = f"{confidence:.2f}"

                    if save_csv:
                        write_to_csv(p.name, label, confidence_str)

                    if save_txt:  # Write to file
                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)
                                          ) / gn).view(-1).tolist()  # normalized xywh
                        # label format
                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)
                        with open(f"{txt_path}.txt", "a") as f:
                            f.write(("%g " * len(line)).rstrip() % line + "\n")

                    if save_img or save_crop or view_img:  # Add bbox to image
                        c = int(cls)  # integer class
                        label = None if hide_labels else (
                            names[c] if hide_conf else f"{names[c]} {conf:.2f}")
                        annotator.box_label(xyxy, label, color=colors(c, True))
                    if save_crop:
                        save_one_box(xyxy, imc, file=save_dir / "crops" /
                                     names[c] / f"{p.stem}.jpg", BGR=True)

            # Stream results
            im0 = annotator.result()
            if view_img:
                if platform.system() == "Linux" and p not in windows:
                    windows.append(p)
                    # allow window resize (Linux)
                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL |
                                    cv2.WINDOW_KEEPRATIO)
                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])
                cv2.imshow(str(p), im0)
                cv2.waitKey(1)  # 1 millisecond

            # Save results (image with detections)
            if save_img:
                if dataset.mode == "image":
                    cv2.imwrite(save_path, im0)
                else:  # 'video' or 'stream'
                    if vid_path[i] != save_path:  # new video
                        vid_path[i] = save_path
                        if isinstance(vid_writer[i], cv2.VideoWriter):
                            # release previous video writer
                            vid_writer[i].release()
                        if vid_cap:  # video
                            fps = vid_cap.get(cv2.CAP_PROP_FPS)
                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        else:  # stream
                            fps, w, h = 30, im0.shape[1], im0.shape[0]
                        # force *.mp4 suffix on results videos
                        save_path = str(Path(save_path).with_suffix(".mp4"))
                        vid_writer[i] = cv2.VideoWriter(
                            save_path, cv2.VideoWriter_fourcc(*"mp4v"), fps, (w, h))
                    vid_writer[i].write(im0)

        # Print time (inference-only)
        LOGGER.info(
            f"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1E3:.1f}ms")

    # Print results
    t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image
    LOGGER.info(
        f"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}" % t)
    if save_txt or save_img:
        s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ""
        LOGGER.info(f"Results saved to {colorstr('bold', save_dir)}{s}")
    if update:
        # update model (to fix SourceChangeWarning)
        strip_optimizer(weights[0])


def parse_opt():
    """Parses command-line arguments for YOLOv5 detection, setting inference options and model configurations."""
    parser = argparse.ArgumentParser()
    parser.add_argument("--weights", nargs="+", type=str, default=ROOT/"yolov5s.pt", help="model path or triton URL")
    parser.add_argument("--source", type=str, default=ROOT/"data/images", help="file/dir/URL/glob/screen/0(webcam)")  #ROOT/"data/images"
    parser.add_argument("--data", type=str, default=ROOT/"data/coco128.yaml", help="(optional) dataset.yaml path")
    parser.add_argument("--imgsz", "--img", "--img-size", nargs="+", type=int, default=[640], help="inference size h,w")
    parser.add_argument("--conf-thres", type=float, default=0.25, help="confidence threshold")
    parser.add_argument("--iou-thres", type=float, default=0.45, help="NMS IoU threshold")
    parser.add_argument("--max-det", type=int, default=1000, help="maximum detections per image")
    parser.add_argument("--device", default="", help="cuda device, i.e. 0 or 0,1,2,3 or cpu")
    parser.add_argument("--view-img", action="store_true", help="show results")
    parser.add_argument("--save-txt", action="store_true", help="save results to *.txt")
    parser.add_argument("--save-csv", action="store_true", help="save results in CSV format")
    parser.add_argument("--save-conf", action="store_true", help="save confidences in --save-txt labels")
    parser.add_argument("--save-crop", action="store_true", help="save cropped prediction boxes")
    parser.add_argument("--nosave", action="store_true", help="do not save images/videos")
    parser.add_argument("--classes", nargs="+", type=int, help="filter by class: --classes 0, or --classes 0 2 3")
    parser.add_argument("--agnostic-nms", action="store_true", help="class-agnostic NMS")
    parser.add_argument("--augment", action="store_true", help="augmented inference")
    parser.add_argument("--visualize", action="store_true", help="visualize features")
    parser.add_argument("--update", action="store_true", help="update all models")
    parser.add_argument("--project", default=ROOT / "runs/detect", help="save results to project/name")
    parser.add_argument("--name", default="exp", help="save results to project/name")
    parser.add_argument("--exist-ok", action="store_true", help="existing project/name ok, do not increment")
    parser.add_argument("--line-thickness", default=3, type=int, help="bounding box thickness (pixels)")
    parser.add_argument("--hide-labels", default=False, action="store_true", help="hide labels")
    parser.add_argument("--hide-conf", default=False, action="store_true", help="hide confidences")
    parser.add_argument("--half", action="store_true", help="use FP16 half-precision inference")
    parser.add_argument("--dnn", action="store_true", help="use OpenCV DNN for ONNX inference")
    parser.add_argument("--vid-stride", type=int, default=1, help="video frame-rate stride")
    opt = parser.parse_args()
    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand å°†imgszçš„ä¿¡æ¯æ‰©å±•æˆhÃ—w
    print_args(vars(opt))
    return opt


def main(opt):
    """Executes YOLOv5 model inference with given options, checking requirements before running the model."""
    # æ£€æŸ¥ç¯å¢ƒï¼Œæ‰“å°å‚æ•°
    check_requirements(ROOT / "requirements.txt",
                       exclude=("tensorboard", "thop"))
    # æ‰§è¡Œrunå‡½æ•°
    run(**vars(opt))
    # **vars(opt): è¿™æ˜¯ Python ä¸­çš„è§£åŒ…æ“ä½œç¬¦ï¼Œç§°ä¸ºå…³é”®å­—å‚æ•°è§£åŒ…ã€‚å®ƒå°†å­—å…¸ä¸­çš„é”®å€¼å¯¹ä½œä¸ºå…³é”®å­—å‚æ•°ä¼ é€’ç»™å‡½æ•°ã€‚
    # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå­—å…¸ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹ä¼šä½œä¸ºç‹¬ç«‹çš„å‚æ•°ä¼ é€’ç»™ run å‡½æ•°ã€‚


if __name__ == "__main__":
    opt = parse_opt()
    main(opt)
